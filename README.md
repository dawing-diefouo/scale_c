<<<<<<< HEAD
# ğŸ§  H5P-Generator mit TinyLlama + LoRA (CPU-freundlich)

Dieses Projekt finetuned ein LLM (TinyLlama-1.1B-Chat) darauf, **valide H5P Multiple-Choice content.json Dateien** automatisch zu generieren.
Es beinhaltet:

* Extraktion von content.json aus bestehenden .h5p Dateien
* Erzeugung von Instruction-Pairs
* Training mit LoRA auf CPU
* Validierte JSON-Ausgabe
* Erstellung fertiger .h5p Pakete

---

# ğŸ“‚ Projektstruktur

```
scale_c/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Originale .h5p Dateien
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚      â”œâ”€â”€ train_data.jsonl  # TrainingsdatensÃ¤tze
â”‚   â””â”€â”€ h5p/                # Hier entstehen generierte .h5p Dateien
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ final_model_cpu/    # Fine-Tuned Modell + Tokenizer
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_h5p.py      # Extrahiert content.json aus raw .h5p
â”‚   â”œâ”€â”€ preprocessing.py    # erstellt Chat-Prompt + tokenisiert
â”‚   â”œâ”€â”€ model_setup.py      # lÃ¤dt Modell + konfiguriert LoRA
â”‚   â”œâ”€â”€ trainer.py          # Training-Loop
â”‚   â”œâ”€â”€ data_loader.py      # lÃ¤dt Dataset
â”‚   â”œâ”€â”€ h5p_validator.py    # prÃ¼ft JSON-Struktur
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ train.py            # Haupt-Trainingsskript
â”‚
â””â”€â”€ test_model_cpu.py       # Inference + H5P-Generierung
```

---

# ğŸ“¥ 1. Datenerstellung aus bestehenden H5P Dateien

Alle H5P-Quellen in:

```
data/raw/*.h5p
```

Dann ausfÃ¼hren:

```bash
python src/extract_h5p.py
```

Ergebnis:

```
data/processed/train_data.jsonl
```

Jede Zeile enthÃ¤lt:

```json
{
  "instruction": "...",
  "output": "{json-string}"
}
```

---

# ğŸ›  2. Finetuning (TinyLlama + LoRA)

Training starten:

```bash
python -m src.train
```

Das Skript:

* lÃ¤dt train_data.jsonl
* erstellt Chat-Prompts:

```
<|system|>Du bist ein H5P-Generator...</s>
<|user|>Instruction</s>
<|assistant|>OutputJSON</s>
```

* tokenisiert
* fÃ¼hrt LoRA-Training durch
* speichert alles nach:

```
outputs/final_model_cpu/
```

---

# âš™ï¸ 3. Hyperparameter (einfach erklÃ¤rt)

### **num_epochs = 3**

Wie oft das Modell alle Trainingsdaten sieht.
Mehr Daten â†’ weniger Epochen notwendig.

### **learning_rate = 1e-4**

Wie stark das Modell bei jedem Schritt lernt.
Niedriger = stabiler, besser fÃ¼r JSON-Aufgaben.

### **warmup_steps = 200**

Modell beginnt mit kleiner Lernrate â†’ schÃ¼tzt vor instabilem Training.

### **batch_size = 1**

Notwendig auf CPU.

### **gradient_accumulation_steps = 4**

Simuliert effektiv Batch-Size 4 â†’ stabilisiert das Training.

### **max_length = 1024**

H5P content.json sind lang â†’ 1024 Tokens optimal.

---

# ğŸ”§ LoRA-Parameter

### **r = 16**

LernkapazitÃ¤t der Adapter.

### **alpha = 32**

Skalierung der LoRA-Updates.

### **dropout = 0.1**

Verhindert Overfitting bei kleinen DatensÃ¤tzen.

### **target_modules**

LoRA wird in folgenden Llama-Modulen aktiv:

```
q_proj, k_proj, v_proj, o_proj,
gate_proj, up_proj, down_proj
```

---

# ğŸ§ª 4. Inference â€“ Erzeuge ein valides H5P content.json

Mit:

```bash
python test_model_cpu.py
```

Der Prompt nutzt exakt dasselbe Format wie im Training:

```
<|system|>Du bist ein H5P-Generator...</s>
<|user|>Frage...</s>
<|assistant|>
```

Das Modell erzeugt:

* valides JSON
* H5P-Struktur mit question, answers, behaviour, overallFeedback
* validiert durch `H5PValidator`
* speichert fertiges H5P-Paket in:

```
data/h5p/generated_mc.h5p
```

---

# ğŸ“¦ 5. Erzeugte H5P Datei Ã¶ffnen

Du kannst die Datei direkt testen auf:

ğŸ‘‰ [https://h5p.org/multichoice](https://h5p.org/multichoice)

Einfach **Upload** wÃ¤hlen.

---

# ğŸš€ 6. Empfehlung fÃ¼r gute Ergebnisse

Damit TinyLlama verlÃ¤sslich gÃ¼ltige H5P-JSON generiert:

* **mindestens 200 TrainingsdatensÃ¤tze** verwenden
* **Epochen auf 3 reduzieren** (sonst Overfitting)
* **do_sample=False** und **temperature=0.0** beim Inference setzen
* **Validator verwenden**, um Fehler sofort zu erkennen

Mit 200 Beispielen wird die QualitÃ¤t **dramatisch** besser.

---

# ğŸ¤ Weiterentwicklung

Empfohlene Erweiterungen:

* Automatischer H5P-Kursgenerator
* Weitere H5P-Typen (Drag&Drop, Fill in the Blanks)
* Auto-Augmentation fÃ¼r mehr Trainingsdaten
* Web-Interface (Gradio/Streamlit)

---

MIT License â€“ frei erweiterbar.
=======
# ğŸ§  CyberSecurity Instruction-Finetuning  
**Fine-Tuning eines Sprachmodells mit H5P-Lerninhalten (Instruction-FT + PEFT/LoRA)**

![Python](https://img.shields.io/badge/Python-3.10+-blue)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![PEFT](https://img.shields.io/badge/LoRA-Adapter-green)
![Status](https://img.shields.io/badge/Status-Research%20Prototype-orange)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

---

## ğŸ§© ProjektÃ¼bersicht
Dieses Projekt untersucht, wie **H5P-Lerninhalte** (z. B. Quizfragen) genutzt werden kÃ¶nnen,  
um ein **Sprachmodell** mit **Instruction-Finetuning** und **Parameter-Efficient Fine-Tuning (PEFT/LoRA)** zu verbessern.  
Das Beispielthema ist *Cybersicherheit*.

Ziel: Ein Modell, das Lernfragen beantworten und erklÃ¤ren kann â€“ auf Basis realer H5P-Daten.

---

## ğŸ—‚ï¸ Projektstruktur

```bash
project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Original H5P-Dateien
â”‚   â”œâ”€â”€ processed/          # Extrahierte JSONs
â”‚   â””â”€â”€ dataset.jsonl       # Finale Trainingsdaten fÃ¼r Instruction-FT
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_h5p.py      # Skript: H5P â†’ JSONL
â”‚   â”œâ”€â”€ train_instruction_ft.py  # Training mit HuggingFace + PEFT
â”‚   â””â”€â”€ utils/              # Hilfsfunktionen
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_preview.ipynb  # Dateninspektion
â”‚   â””â”€â”€ training_eval.ipynb # Evaluation des Trainings
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ training_config.yaml
â”‚   â””â”€â”€ model_config.json
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ checkpoints/        # Modellgewichte
â”‚   â”œâ”€â”€ logs/               # TensorBoard / W&B Logs
â”‚   â””â”€â”€ eval_results.json
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
>>>>>>> 4883651dd8b773f588b5de801d971fe629bcefe2
