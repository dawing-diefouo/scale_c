2025-11-18 03:16:45,139 - INFO - ============================================================
2025-11-18 03:16:45,139 - INFO - üéì H5P-Generator Training
2025-11-18 03:16:45,139 - INFO - ============================================================
2025-11-18 03:16:45,139 - INFO - Modell: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:16:45,139 - INFO - Dataset: data\processed\train_data.jsonl
2025-11-18 03:16:45,139 - INFO - Output: outputs\final_model_cpu
2025-11-18 03:16:45,140 - INFO - Max Length: 1024
2025-11-18 03:16:45,140 - INFO - Batch Size: 1
2025-11-18 03:16:45,140 - INFO - Gradient Accumulation: 4
2025-11-18 03:16:45,140 - INFO - Effektive Batch Size: 4
2025-11-18 03:16:45,140 - INFO - Epochen: 5
2025-11-18 03:16:45,140 - INFO - Learning Rate: 0.0002
2025-11-18 03:16:45,141 - INFO - ============================================================
2025-11-18 03:16:45,142 - INFO - ‚úì Konfiguration gespeichert
2025-11-18 03:16:45,142 - INFO - üì• Lade Training: data\processed\train_data.jsonl
2025-11-18 03:16:46,030 - ERROR - ‚ùå Fehler: No module named 'utils'
Traceback (most recent call last):
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\train.py", line 40, in main
    train_dataset = data_loader.load_train_data()
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\data_loader.py", line 24, in load_train_data
    self._validate_dataset(dataset)
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\data_loader.py", line 59, in _validate_dataset
    from utils import is_valid_json
ModuleNotFoundError: No module named 'utils'
2025-11-18 03:17:53,977 - INFO - ============================================================
2025-11-18 03:17:53,977 - INFO - üéì H5P-Generator Training
2025-11-18 03:17:53,977 - INFO - ============================================================
2025-11-18 03:17:53,977 - INFO - Modell: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:17:53,977 - INFO - Dataset: data\processed\train_data.jsonl
2025-11-18 03:17:53,977 - INFO - Output: outputs\final_model_cpu
2025-11-18 03:17:53,978 - INFO - Max Length: 1024
2025-11-18 03:17:53,978 - INFO - Batch Size: 1
2025-11-18 03:17:53,978 - INFO - Gradient Accumulation: 4
2025-11-18 03:17:53,978 - INFO - Effektive Batch Size: 4
2025-11-18 03:17:53,978 - INFO - Epochen: 5
2025-11-18 03:17:53,978 - INFO - Learning Rate: 0.0002
2025-11-18 03:17:53,978 - INFO - ============================================================
2025-11-18 03:17:53,979 - INFO - ‚úì Konfiguration gespeichert
2025-11-18 03:17:53,979 - INFO - üì• Lade Training: data\processed\train_data.jsonl
2025-11-18 03:17:55,739 - INFO - ‚úì 9 Beispiele geladen
2025-11-18 03:17:55,739 - INFO - üìù Beispiel-Instruction: Erstelle eine H5P-Multiple-Choice-Frage mit 4 Antwortm√∂glichkeiten und 1 richtigen Antwort(en), basi...
2025-11-18 03:17:55,739 - INFO - üìù Beispiel-Output: {"question": "Was ist ein VPN?", "answers": [{"text": "Ein Virus-Schutz", "correct": false}, {"text"...
2025-11-18 03:17:55,739 - ERROR - ‚ùå Fehler: 'NoneType' object has no attribute 'exists'
Traceback (most recent call last):
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\train.py", line 41, in main
    eval_dataset = data_loader.load_eval_data()
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\data_loader.py", line 36, in load_eval_data
    if not self.config.eval_path.exists() or not self.config.eval_path:
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'exists'
2025-11-18 03:19:36,689 - INFO - ============================================================
2025-11-18 03:19:36,689 - INFO - üéì H5P-Generator Training
2025-11-18 03:19:36,689 - INFO - ============================================================
2025-11-18 03:19:36,690 - INFO - Modell: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:19:36,690 - INFO - Dataset: data\processed\train_data.jsonl
2025-11-18 03:19:36,690 - INFO - Output: outputs\final_model_cpu
2025-11-18 03:19:36,690 - INFO - Max Length: 1024
2025-11-18 03:19:36,690 - INFO - Batch Size: 1
2025-11-18 03:19:36,690 - INFO - Gradient Accumulation: 4
2025-11-18 03:19:36,690 - INFO - Effektive Batch Size: 4
2025-11-18 03:19:36,690 - INFO - Epochen: 5
2025-11-18 03:19:36,690 - INFO - Learning Rate: 0.0002
2025-11-18 03:19:36,690 - INFO - ============================================================
2025-11-18 03:19:36,691 - INFO - ‚úì Konfiguration gespeichert
2025-11-18 03:19:36,691 - INFO - üì• Lade Training: data\processed\train_data.jsonl
2025-11-18 03:19:37,515 - INFO - ‚úì 9 Beispiele geladen
2025-11-18 03:19:37,516 - INFO - üìù Beispiel-Instruction: Erstelle eine H5P-Multiple-Choice-Frage mit 4 Antwortm√∂glichkeiten und 1 richtigen Antwort(en), basi...
2025-11-18 03:19:37,516 - INFO - üìù Beispiel-Output: {"question": "Was ist ein VPN?", "answers": [{"text": "Ein Virus-Schutz", "correct": false}, {"text"...
2025-11-18 03:19:37,516 - INFO - ‚ÑπÔ∏è Keine Evaluationsdaten konfiguriert (eval_path=None)
2025-11-18 03:19:37,516 - INFO - ‚öôÔ∏è Lade Tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:19:38,210 - INFO - ‚úì Padding-Seite: left
2025-11-18 03:19:38,222 - INFO - ‚úì Vocab Size: 32000
2025-11-18 03:19:38,222 - INFO - ‚úì BOS: <s> (ID: 1)
2025-11-18 03:19:38,223 - INFO - ‚úì EOS: </s> (ID: 2)
2025-11-18 03:19:38,223 - INFO - ‚úì PAD: </s> (ID: 2)
2025-11-18 03:19:38,223 - INFO - ‚öôÔ∏è Lade Modell: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:19:42,643 - INFO - ‚úì Modell geladen auf: cpu
2025-11-18 03:19:42,646 - INFO - ‚úì Model dtype: torch.float32
2025-11-18 03:19:42,646 - INFO - üîß Aktiviere LoRA
2025-11-18 03:19:42,900 - ERROR - ‚ùå Fehler: No module named 'utils'
Traceback (most recent call last):
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\train.py", line 45, in main
    model, tokenizer = model_setup.setup()
                       ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\model_setup.py", line 101, in setup
    model = self.apply_lora(model)
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\model_setup.py", line 85, in apply_lora
    from utils import print_trainable_params
ModuleNotFoundError: No module named 'utils'
2025-11-18 03:22:07,592 - INFO - ============================================================
2025-11-18 03:22:07,592 - INFO - üéì H5P-Generator Training
2025-11-18 03:22:07,592 - INFO - ============================================================
2025-11-18 03:22:07,592 - INFO - Modell: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:22:07,592 - INFO - Dataset: data\processed\train_data.jsonl
2025-11-18 03:22:07,592 - INFO - Output: outputs\final_model_cpu
2025-11-18 03:22:07,592 - INFO - Max Length: 1024
2025-11-18 03:22:07,592 - INFO - Batch Size: 1
2025-11-18 03:22:07,592 - INFO - Gradient Accumulation: 4
2025-11-18 03:22:07,593 - INFO - Effektive Batch Size: 4
2025-11-18 03:22:07,593 - INFO - Epochen: 5
2025-11-18 03:22:07,593 - INFO - Learning Rate: 0.0002
2025-11-18 03:22:07,593 - INFO - ============================================================
2025-11-18 03:22:07,593 - INFO - ‚úì Konfiguration gespeichert
2025-11-18 03:22:07,593 - INFO - üì• Lade Training: data\processed\train_data.jsonl
2025-11-18 03:22:08,326 - INFO - ‚úì 9 Beispiele geladen
2025-11-18 03:22:08,326 - INFO - üìù Beispiel-Instruction: Erstelle eine H5P-Multiple-Choice-Frage mit 4 Antwortm√∂glichkeiten und 1 richtigen Antwort(en), basi...
2025-11-18 03:22:08,327 - INFO - üìù Beispiel-Output: {"question": "Was ist ein VPN?", "answers": [{"text": "Ein Virus-Schutz", "correct": false}, {"text"...
2025-11-18 03:22:08,327 - INFO - ‚ÑπÔ∏è Keine Evaluationsdaten konfiguriert (eval_path=None)
2025-11-18 03:22:08,327 - INFO - ‚öôÔ∏è Lade Tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:22:09,032 - INFO - ‚úì Padding-Seite: left
2025-11-18 03:22:09,044 - INFO - ‚úì Vocab Size: 32000
2025-11-18 03:22:09,045 - INFO - ‚úì BOS: <s> (ID: 1)
2025-11-18 03:22:09,045 - INFO - ‚úì EOS: </s> (ID: 2)
2025-11-18 03:22:09,045 - INFO - ‚úì PAD: </s> (ID: 2)
2025-11-18 03:22:09,045 - INFO - ‚öôÔ∏è Lade Modell: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:22:12,053 - INFO - ‚úì Modell geladen auf: cpu
2025-11-18 03:22:12,054 - INFO - ‚úì Model dtype: torch.float32
2025-11-18 03:22:12,054 - INFO - üîß Aktiviere LoRA
2025-11-18 03:22:12,275 - INFO - ‚úì Trainierbare Parameter: 12,615,680 / 1,112,664,064 (1.13%)
2025-11-18 03:22:12,276 - INFO - ‚úì LoRA Rank: 16
2025-11-18 03:22:12,276 - INFO - ‚úì LoRA Alpha: 32
2025-11-18 03:22:12,276 - INFO - ‚úì Target Modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-11-18 03:22:12,276 - INFO - üßπ Tokenisiere Trainingsdaten...
2025-11-18 03:22:12,414 - INFO - ‚úì Training tokenisiert: 9 Beispiele
2025-11-18 03:22:12,414 - INFO - üöÄ Starte Training
2025-11-18 03:22:12,415 - ERROR - ‚ùå Fehler: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
Traceback (most recent call last):
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\train.py", line 62, in main
    trainer_instance.train(model, tokenizer, train_tokenized, eval_tokenized)
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\trainer.py", line 74, in train
    training_args = self.create_training_args(has_eval=eval_dataset is not None)
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\trainer.py", line 23, in create_training_args
    return TrainingArguments(
        output_dir=str(self.config.output_dir),
    ...<41 lines>...
        dataloader_num_workers=0,
    )
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-11-18 03:25:21,408 - INFO - ============================================================
2025-11-18 03:25:21,409 - INFO - üéì H5P-Generator Training
2025-11-18 03:25:21,409 - INFO - ============================================================
2025-11-18 03:25:21,409 - INFO - Modell: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:25:21,409 - INFO - Dataset: data\processed\train_data.jsonl
2025-11-18 03:25:21,409 - INFO - Output: outputs\final_model_cpu
2025-11-18 03:25:21,409 - INFO - Max Length: 1024
2025-11-18 03:25:21,409 - INFO - Batch Size: 1
2025-11-18 03:25:21,409 - INFO - Gradient Accumulation: 4
2025-11-18 03:25:21,409 - INFO - Effektive Batch Size: 4
2025-11-18 03:25:21,409 - INFO - Epochen: 5
2025-11-18 03:25:21,409 - INFO - Learning Rate: 0.0002
2025-11-18 03:25:21,409 - INFO - ============================================================
2025-11-18 03:25:21,410 - INFO - ‚úì Konfiguration gespeichert
2025-11-18 03:25:21,410 - INFO - üì• Lade Training: data\processed\train_data.jsonl
2025-11-18 03:25:22,120 - INFO - ‚úì 9 Beispiele geladen
2025-11-18 03:25:22,120 - INFO - üìù Beispiel-Instruction: Erstelle eine H5P-Multiple-Choice-Frage mit 4 Antwortm√∂glichkeiten und 1 richtigen Antwort(en), basi...
2025-11-18 03:25:22,120 - INFO - üìù Beispiel-Output: {"question": "Was ist ein VPN?", "answers": [{"text": "Ein Virus-Schutz", "correct": false}, {"text"...
2025-11-18 03:25:22,120 - INFO - ‚ÑπÔ∏è Keine Evaluationsdaten konfiguriert (eval_path=None)
2025-11-18 03:25:22,120 - INFO - ‚öôÔ∏è Lade Tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:25:22,859 - INFO - ‚úì Padding-Seite: left
2025-11-18 03:25:22,864 - INFO - ‚úì Vocab Size: 32000
2025-11-18 03:25:22,865 - INFO - ‚úì BOS: <s> (ID: 1)
2025-11-18 03:25:22,865 - INFO - ‚úì EOS: </s> (ID: 2)
2025-11-18 03:25:22,865 - INFO - ‚úì PAD: </s> (ID: 2)
2025-11-18 03:25:22,865 - INFO - ‚öôÔ∏è Lade Modell: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:25:25,789 - INFO - ‚úì Modell geladen auf: cpu
2025-11-18 03:25:25,791 - INFO - ‚úì Model dtype: torch.float32
2025-11-18 03:25:25,792 - INFO - üîß Aktiviere LoRA
2025-11-18 03:25:26,031 - INFO - ‚úì Trainierbare Parameter: 12,615,680 / 1,112,664,064 (1.13%)
2025-11-18 03:25:26,032 - INFO - ‚úì LoRA Rank: 16
2025-11-18 03:25:26,032 - INFO - ‚úì LoRA Alpha: 32
2025-11-18 03:25:26,032 - INFO - ‚úì Target Modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-11-18 03:25:26,032 - INFO - üßπ Tokenisiere Trainingsdaten...
2025-11-18 03:25:26,075 - INFO - ‚úì Training tokenisiert: 9 Beispiele
2025-11-18 03:25:26,075 - INFO - üöÄ Starte Training
2025-11-18 03:25:36,968 - ERROR - ‚ùå Training-Fehler: element 0 of tensors does not require grad and does not have a grad_fn
Traceback (most recent call last):
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\trainer.py", line 92, in train
    trainer.train()
    ~~~~~~~~~~~~~^^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\transformers\trainer.py", line 2325, in train
    return inner_training_loop(
        args=args,
    ...<2 lines>...
        ignore_keys_for_eval=ignore_keys_for_eval,
    )
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\transformers\trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\transformers\trainer.py", line 4071, in training_step
    self.accelerator.backward(loss, **kwargs)
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\accelerate\accelerator.py", line 2740, in backward
    loss.backward(**kwargs)
    ~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\torch\_tensor.py", line 625, in backward
    torch.autograd.backward(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        self, gradient, retain_graph, create_graph, inputs=inputs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\torch\autograd\__init__.py", line 354, in backward
    _engine_run_backward(
    ~~~~~~~~~~~~~~~~~~~~^
        tensors,
        ^^^^^^^^
    ...<5 lines>...
        accumulate_grad=True,
        ^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\torch\autograd\graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        t_outputs, *args, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )  # Calls into the C++ engine to run the backward pass
    ^
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
2025-11-18 03:25:36,984 - ERROR - ‚ùå Fehler: element 0 of tensors does not require grad and does not have a grad_fn
Traceback (most recent call last):
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\train.py", line 62, in main
    trainer_instance.train(model, tokenizer, train_tokenized, eval_tokenized)
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\src\trainer.py", line 92, in train
    trainer.train()
    ~~~~~~~~~~~~~^^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\transformers\trainer.py", line 2325, in train
    return inner_training_loop(
        args=args,
    ...<2 lines>...
        ignore_keys_for_eval=ignore_keys_for_eval,
    )
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\transformers\trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\transformers\trainer.py", line 4071, in training_step
    self.accelerator.backward(loss, **kwargs)
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\accelerate\accelerator.py", line 2740, in backward
    loss.backward(**kwargs)
    ~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\torch\_tensor.py", line 625, in backward
    torch.autograd.backward(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        self, gradient, retain_graph, create_graph, inputs=inputs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\torch\autograd\__init__.py", line 354, in backward
    _engine_run_backward(
    ~~~~~~~~~~~~~~~~~~~~^
        tensors,
        ^^^^^^^^
    ...<5 lines>...
        accumulate_grad=True,
        ^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\.venv\Lib\site-packages\torch\autograd\graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        t_outputs, *args, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )  # Calls into the C++ engine to run the backward pass
    ^
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
2025-11-18 03:29:21,965 - INFO - ============================================================
2025-11-18 03:29:21,965 - INFO - üéì H5P-Generator Training
2025-11-18 03:29:21,965 - INFO - ============================================================
2025-11-18 03:29:21,965 - INFO - Modell: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:29:21,965 - INFO - Dataset: data\processed\train_data.jsonl
2025-11-18 03:29:21,966 - INFO - Output: outputs\final_model_cpu
2025-11-18 03:29:21,966 - INFO - Max Length: 1024
2025-11-18 03:29:21,966 - INFO - Batch Size: 1
2025-11-18 03:29:21,966 - INFO - Gradient Accumulation: 4
2025-11-18 03:29:21,966 - INFO - Effektive Batch Size: 4
2025-11-18 03:29:21,966 - INFO - Epochen: 5
2025-11-18 03:29:21,966 - INFO - Learning Rate: 0.0002
2025-11-18 03:29:21,966 - INFO - ============================================================
2025-11-18 03:29:21,967 - INFO - ‚úì Konfiguration gespeichert
2025-11-18 03:29:21,967 - INFO - üì• Lade Training: data\processed\train_data.jsonl
2025-11-18 03:29:22,638 - INFO - ‚úì 9 Beispiele geladen
2025-11-18 03:29:22,638 - INFO - üìù Beispiel-Instruction: Erstelle eine H5P-Multiple-Choice-Frage mit 4 Antwortm√∂glichkeiten und 1 richtigen Antwort(en), basi...
2025-11-18 03:29:22,639 - INFO - üìù Beispiel-Output: {"question": "Was ist ein VPN?", "answers": [{"text": "Ein Virus-Schutz", "correct": false}, {"text"...
2025-11-18 03:29:22,639 - INFO - ‚ÑπÔ∏è Keine Evaluationsdaten konfiguriert (eval_path=None)
2025-11-18 03:29:22,639 - INFO - ‚öôÔ∏è Lade Tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:29:23,255 - INFO - ‚úì Padding-Seite: left
2025-11-18 03:29:23,263 - INFO - ‚úì Vocab Size: 32000
2025-11-18 03:29:23,263 - INFO - ‚úì BOS: <s> (ID: 1)
2025-11-18 03:29:23,264 - INFO - ‚úì EOS: </s> (ID: 2)
2025-11-18 03:29:23,264 - INFO - ‚úì PAD: </s> (ID: 2)
2025-11-18 03:29:23,264 - INFO - ‚öôÔ∏è Lade Modell: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-18 03:29:26,227 - INFO - ‚úì Modell geladen auf: cpu
2025-11-18 03:29:26,227 - INFO - ‚úì Model dtype: torch.float32
2025-11-18 03:29:26,228 - INFO - üîß Aktiviere LoRA
2025-11-18 03:29:26,433 - INFO - ‚úì Trainierbare Parameter: 12,615,680 / 1,112,664,064 (1.13%)
2025-11-18 03:29:26,433 - INFO - ‚úì LoRA Rank: 16
2025-11-18 03:29:26,434 - INFO - ‚úì LoRA Alpha: 32
2025-11-18 03:29:26,434 - INFO - ‚úì Target Modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-11-18 03:29:26,434 - INFO - üßπ Tokenisiere Trainingsdaten...
2025-11-18 03:29:26,480 - INFO - ‚úì Training tokenisiert: 9 Beispiele
2025-11-18 03:29:26,480 - INFO - üöÄ Starte Training
2025-11-18 03:51:39,738 - INFO - ‚úì Training erfolgreich abgeschlossen
2025-11-18 03:51:39,747 - INFO - üíæ Speichere Modell
2025-11-18 03:51:40,423 - INFO - ‚úì Stats gespeichert: outputs\final_model_cpu\training_stats.json
2025-11-18 03:51:40,423 - INFO - üéâ Training fertig! ‚Üí outputs\final_model_cpu
2025-11-18 03:51:40,426 - INFO - ============================================================
2025-11-18 03:51:40,426 - INFO - üéâ Training erfolgreich abgeschlossen!
2025-11-18 03:51:40,426 - INFO - ============================================================
2025-11-18 03:51:40,427 - INFO - üìÅ Modell: C:\Users\dawin\OneDrive\Documents\Semester_1\Projekt2\scale_c\outputs\final_model_cpu
2025-11-18 03:51:40,428 - INFO - üìä Trainingsdaten: 9 Beispiele
2025-11-18 03:51:40,428 - INFO - üîß LoRA Rank: 16
2025-11-18 03:51:40,428 - INFO - üìà Epochen: 5
2025-11-18 03:51:40,428 - INFO - ============================================================
2025-11-18 03:51:40,428 - INFO - N√§chste Schritte:
2025-11-18 03:51:40,428 - INFO - 1. Teste das Modell: python inference.py
2025-11-18 03:51:40,428 - INFO - 2. Validiere generierte H5Ps
2025-11-18 03:51:40,428 - INFO - 3. Bei Bedarf: Nachtraining mit mehr Daten
2025-11-18 03:51:40,429 - INFO - ============================================================
