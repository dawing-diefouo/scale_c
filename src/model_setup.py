from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
import torch
import logging

from src.config import ModelConfig, LoRAConfig


class ModelSetup:
    """K√ºmmert sich um Modell- und Tokenizer-Setup"""

    def __init__(self, model_config: ModelConfig, lora_config: LoRAConfig, logger: logging.Logger):
        self.model_config = model_config
        self.lora_config = lora_config
        self.logger = logger

    def load_tokenizer(self):
        """L√§dt und konfiguriert Tokenizer"""
        self.logger.info(f"‚öôÔ∏è Lade Tokenizer: {self.model_config.base_model}")

        tokenizer = AutoTokenizer.from_pretrained(
            self.model_config.base_model,
            trust_remote_code=self.model_config.trust_remote_code
        )

        # PAD Token Setup f√ºr TinyLlama
        if tokenizer.pad_token is None:
            if tokenizer.unk_token is not None:
                tokenizer.pad_token = tokenizer.unk_token
                self.logger.info(f"‚úì pad_token = unk_token ({tokenizer.unk_token})")
            else:
                tokenizer.pad_token = tokenizer.eos_token
                self.logger.info(f"‚úì pad_token = eos_token ({tokenizer.eos_token})")

        # Padding-Seite f√ºr Llama
        tokenizer.padding_side = self.model_config.padding_side
        self.logger.info(f"‚úì Padding-Seite: {tokenizer.padding_side}")

        # Token-Info
        self.logger.info(f"‚úì Vocab Size: {len(tokenizer)}")
        self.logger.info(f"‚úì BOS: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
        self.logger.info(f"‚úì EOS: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
        self.logger.info(f"‚úì PAD: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")

        return tokenizer

    def load_model(self):
        """L√§dt Basis-Modell"""
        self.logger.info(f"‚öôÔ∏è Lade Modell: {self.model_config.base_model}")

        model = AutoModelForCausalLM.from_pretrained(
            self.model_config.base_model,
            torch_dtype=torch.float32,
            device_map=None if self.model_config.device_map == "cpu" else self.model_config.device_map,
            low_cpu_mem_usage=False,
            trust_remote_code=self.model_config.trust_remote_code
        )

        # Wichtig f√ºr Training
        model.config.use_cache = False

        self.logger.info(f"‚úì Modell geladen auf: {self.model_config.device_map}")
        self.logger.info(f"‚úì Model dtype: {model.dtype}")

        return model

    def apply_lora(self, model):
        """Wendet LoRA auf Modell an"""
        self.logger.info("üîß Aktiviere LoRA")

        lora_config = LoraConfig(
            r=self.lora_config.r,
            lora_alpha=self.lora_config.alpha,
            lora_dropout=self.lora_config.dropout,
            target_modules=self.lora_config.target_modules,
            task_type=TaskType.CAUSAL_LM,
            bias=self.lora_config.bias
        )

        # Model f√ºr Training vorbereiten
        model = get_peft_model(model, lora_config)

        # Parameter-Statistik
        from src.utils import print_trainable_params
        stats = print_trainable_params(model)
        self.logger.info(
            f"‚úì Trainierbare Parameter: {stats['trainable']:,} / {stats['total']:,} "
            f"({stats['percentage']:.2f}%)"
        )
        self.logger.info(f"‚úì LoRA Rank: {self.lora_config.r}")
        self.logger.info(f"‚úì LoRA Alpha: {self.lora_config.alpha}")
        self.logger.info(f"‚úì Target Modules: {self.lora_config.target_modules}")

        return model

    def setup(self):
        """Komplettes Setup: Tokenizer + Modell + LoRA"""
        tokenizer = self.load_tokenizer()
        model = self.load_model()
        model = self.apply_lora(model)
        return model, tokenizer

